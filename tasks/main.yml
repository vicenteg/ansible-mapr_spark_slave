---
# tasks file for sparkslave

# Assumes the MapR repos are already configured.

- name: install mapr-spark for RedHat/CentOS
  sudo_user: root
  yum: name='{{item}}-{{spark_version}}.{{spark_build}}' state=present
  with_items:
    - mapr-spark
  when: ansible_distribution in ("CentOS", "RedHat", "Amazon")
  environment: proxy_env

- name: install mapr-spark for Debian/Ubuntu
  sudo_user: root
  apt: name='{{item}}-{{spark_version}}.{{spark_build}}' state=present
  with_items:
    - mapr-spark
  when: ansible_distribution in ("Debian", "Ubuntu")
  environment: proxy_env

- name: create a volume for Spark
  run_once: yes
  sudo_user: '{{mapr_admin_username}}'
  mapr_volume: name={{spark_volume_name}} path={{spark_volume_path}} state=present username={{mapr_admin_username}} password={{mapr_password_clear|default("mapr")}} mapr_webserver={{mapr_webserver}}

- name: set spark work directory in spark configuration
  lineinfile: dest=/opt/mapr/spark/spark-{{spark_version}}/conf/spark-env.sh state=present regexp="^export SPARK_WORKER_DIR" line="export SPARK_WORKER_DIR={{spark_work_dir}}"

- name: create a spark work directory
  sudo_user: root
  file: dest={{spark_work_dir}} owner={{mapr_admin_username}} group={{mapr_admin_username}} state=directory mode=0750

- name: create a local volume for spark work directory
  mapr_volume: 
  args:
    name: mapr.{{ansible_fqdn}}.local.spark
    path: /var/mapr/local/{{ansible_fqdn}}/spark
    createparent: true
    replication: "1"
    minreplication: "1"
    localvolumehost: '{{ansible_fqdn}}'
    mapr_webserver: '{{mapr_webserver}}'
    state: present
